<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Dr. David Greenwood">
  <title>Appearance Features</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4/dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4/dist/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@11.2.0/styles/atom-one-dark.min.css">
  <link rel="stylesheet" href="assets/style.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Appearance Features</h1>
  <p class="subtitle">Audiovisual Processing CMP-6026A</p>
  <p class="author">Dr. David Greenwood</p>
</section>

<section>
<section id="content" class="title-slide slide level1">
<h1>Content</h1>
<ul>
<li>DCT Features</li>
<li>Eigenfaces</li>
<li>Appearance Models</li>
<li>Visual Feature Efficacy</li>
<li>Model Fusion</li>
</ul>
</section>
<section id="image-based-features" class="slide level2">
<h2>Image-based Features</h2>
<p>The main limitation of shape-only features is there is a lot of information missing.</p>
<div>
<ul>
<li class="fragment">Modelling only lip-shape discards information about the teeth and tongue for example.</li>
<li class="fragment">Why not use the full <em>appearance</em> of the face?</li>
</ul>
</div>
</section></section>
<section>
<section id="dct-features" class="title-slide slide level1">
<h1>DCT Features</h1>

</section>
<section id="discrete-cosine-transform-dct" class="slide level2">
<h2>Discrete Cosine Transform (DCT)</h2>
<p>Performs a similar function to DFT in that it transforms a signal (or image) from the spatial domain to the frequency domain.</p>
<ul>
<li>The difference is that it only considers the real-valued cosine components of the DFT.</li>
<li>We can compact the energy of the signal into the low frequency bins.</li>
<li>Used in JPEG compression.</li>
<li>First proposed by Nasir Ahmed in 1972.</li>
</ul>
</section>
<section id="review-dft" class="slide level2">
<h2>Review: DFT</h2>
<figure>
<img data-src="assets/img4/lecture_02_slide_13.png" style="width:80.0%" alt="Lecture 2, Slide 13: Fourier Transform" /><figcaption aria-hidden="true">Lecture 2, Slide 13: Fourier Transform</figcaption>
</figure>
</section>
<section id="dct-1d" class="slide level2">
<h2>DCT 1D</h2>
<p><span class="math display">\[
X_k = s(k) ~ \sum_{n=0}^{N-1} x_n
cos \left[ \frac{\pi k (2n + 1)}{2N} \right]
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(X\)</span> is the DCT output</li>
<li><span class="math inline">\(x\)</span> is the input signal</li>
<li><span class="math inline">\(N\)</span> is the number of samples</li>
<li><span class="math inline">\(k = 0, 1, 2, \dots, N-1\)</span></li>
<li><span class="math inline">\(s(0) = \sqrt{1/N}, ~ s(k \neq 0) = \sqrt{2/N}\)</span></li>
</ul>
<aside class="notes">
<p>MATLAB has the dct function</p>
</aside>
</section>
<section class="slide level2">

<h3 id="point-1-d-dct-basis">8-Point 1-D DCT Basis</h3>
<div class="columns">
<div class="column">
<p><img data-src="assets/plots2/basis_vectors_1d.png" /></p>
</div><div class="column">
<p><span class="math display">\[Y_k = cos \left[ \frac{\pi k (2n + 1)}{2N} \right]\]</span></p>
<p><span class="math display">\[n = 0, 1, 2, \dots, N-1\]</span></p>
</div>
</div>
</section>
<section id="dct-for-1d-signals" class="slide level2">
<h2>DCT for 1D Signals</h2>
<p>Let’s look at one 8 x 8 block in an image.</p>
<p><img data-src="assets/plots2/cameraman_block.png" /></p>
</section>
<section class="slide level2">

<figure>
<img data-src="assets/plots2/dct_8x8_block.png" alt="DCT of each row of the image block" /><figcaption aria-hidden="true">DCT of each row of the image block</figcaption>
</figure>
</section>
<section id="dct-for-1d-signals-1" class="slide level2">
<h2>DCT for 1D Signals</h2>
<p><img data-src="assets/plots2/block_row3.png" /></p>
<div>
<ul>
<li class="fragment">Most of the energy is concentrated in the low frequency coefficients.</li>
<li class="fragment">Images have less high frequency information.</li>
</ul>
</div>
</section>
<section id="dct-for-2d-signals" class="slide level2">
<h2>DCT for 2D Signals</h2>
<p>We have only considered vectors so far.</p>
<ul>
<li>Images are 2-dimensional (two spatial co-ordinates).</li>
<li>Apply DCT to both rows and columns of the image.</li>
</ul>
<div style="font-size:0.75em;">
<p><span class="math display">\[
X_{u, v} = s_u s_v ~ \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} I(x, y) ~
    cos \left[ \frac{\pi u (2x + 1)}{2N} \right]
    cos \left[ \frac{\pi v (2y + 1)}{2N} \right]
\]</span></p>
</div>
</section>
<section id="dct-for-2d-signals-1" class="slide level2">
<h2>DCT for 2D Signals</h2>
<div class="columns">
<div class="column">
<p><img data-src="assets/img4/DCT_2D_8x8.png" /></p>
</div><div class="column">
<p>Rather than basis vectors, we have basis images.</p>
</div>
</div>
</section>
<section id="dct-for-2d-signals-2" class="slide level2">
<h2>DCT for 2D Signals</h2>
<p>Let’s look again at the same 8 x 8 block in an image.</p>
<p><img data-src="assets/plots2/cameraman_block.png" /></p>
</section>
<section id="dct-for-2d-signals-3" class="slide level2">
<h2>DCT for 2D Signals</h2>
<p>Here is the 2D DCT of the block.</p>
<p><img data-src="assets/plots2/dct_2d_block.png" /></p>
</section>
<section id="dct-for-2d-signals-4" class="slide level2">
<h2>DCT for 2D Signals</h2>
<div class="columns">
<div class="column">
<p><img data-src="assets/plots2/dct_2d_values.png" /></p>
</div><div class="column">
<p>Let’s examine the actual values of the coefficients.</p>
</div>
</div>
</section>
<section id="dct-for-2d-signals-5" class="slide level2">
<h2>DCT for 2D Signals</h2>
<div class="columns">
<div class="column">
<p><img data-src="assets/plots2/dct_2d_values_high.png" /></p>
</div><div class="column">
<p>Notice that the most significant values congregate at the top left.</p>
</div>
</div>
</section>
<section id="dct-for-2d-signals-6" class="slide level2">
<h2>DCT for 2D Signals</h2>
<div class="columns">
<div class="column">
<p><img data-src="assets/plots2/dct_2d_values_high.png" /></p>
</div><div class="column">
<p>We can stack the top left values to make a feature vector.</p>
<p><span class="math inline">\(f = (972, 85, 19, -59, \dots)\)</span></p>
</div>
</div>
</section>
<section id="dct-for-2d-signals-7" class="slide level2">
<h2>DCT for 2D Signals</h2>
<div class="columns">
<div class="column">
<p><img data-src="assets/plots2/dct_2d_values_low.png" /></p>
</div><div class="column">
<p>If we want to reconstruct the image using the inverse DCT, we can set the low values to zero to view the reconstruction loss.</p>
</div>
</div>
</section>
<section id="dct-for-2d-signals-8" class="slide level2">
<h2>DCT for 2D Signals</h2>
<div class="columns">
<div class="column">
<p><img data-src="assets/plots2/dct_2d_values_zeroed.png" /></p>
</div><div class="column">
<p>Here you can see we have zeroed the lower right triangle.</p>
<p>You should decide empirically how many coefficients to retain. Often, many fewer than half produce good results.</p>
</div>
</div>
</section>
<section id="dct-features-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">DCT Features</h2>
<p>One approach for modelling the appearance of the face:</p>
<div>
<ul>
<li class="fragment">Convert the image to greyscale.</li>
<li class="fragment">Crop the image to contain only the region of interest (the mouth).</li>
<li class="fragment">Normalise the size of the image to some default size (the images need the same number of pixels in each frame).</li>
<li class="fragment">Either resize the cropped regions, or better, use a constant clipping box.</li>
</ul>
</div>
</section>
<section id="dct-features-2" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">DCT Features</h2>
<p>One approach for modelling the appearance of the face:</p>
<div>
<ul>
<li class="fragment">Segment the region into n x n pixel blocks.</li>
<li class="fragment">Experiment with <span class="math inline">\(1 \leq n \leq 8\)</span>.</li>
<li class="fragment">Apply a 2D Discrete Cosine Transform (DCT) to each block.</li>
<li class="fragment">Extract coefficients that encode low frequency information.</li>
</ul>
</div>
</section>
<section id="d-dct-case-study" class="slide level2">
<h2>2D DCT case study</h2>
<p><img data-src="assets/plots2/resize_mouth.png" /></p>
<p>A region of interest is cropped, resized and converted to greyscale.</p>
</section>
<section id="d-dct-case-study-1" class="slide level2">
<h2>2D DCT case study</h2>
<p><img data-src="assets/plots2/restored_mouth.png" /></p>
<p>From the greyscale image, we can extract the DCT coefficients. We retain only the low frequency coefficients, and show a reconstruction of the image.</p>
</section>
<section id="d-dct-case-study-2" class="slide level2">
<h2>2D DCT case study</h2>
<p><img data-src="assets/plots2/restored_compare.png" /></p>
<p>Perceptual evaluations of the reconstruction are informative, but your experiments should determine how useful the features are for recognising speech.</p>
</section></section>
<section>
<section id="eigenfaces" class="title-slide slide level1">
<h1>Eigenfaces</h1>

</section>
<section id="eigenfaces-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<div>
<ul>
<li class="fragment">Crop the images to contain only the region of interest.</li>
<li class="fragment">Normalise the size of the image.</li>
<li class="fragment">Images need the <strong>same number</strong> of pixels in each frame.</li>
<li class="fragment">Resize the images, or better, use a constant clipping box.</li>
</ul>
</div>
</section>
<section id="eigenfaces-2" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<div>
<ul>
<li class="fragment">Apply <strong>PCA</strong> to the size-normalised images.</li>
<li class="fragment">When applied to face images, referred to as <strong>Eigenfaces</strong>.</li>
<li class="fragment">This was the basis of an early face recognition system. (Turk and Pentland, 1990).</li>
</ul>
</div>
<aside class="notes">
<p>Eigen faces is just a term - PCA has no understanding of the type of data being processed.</p>
</aside>
</section>
<section id="eigenfaces-3" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<figure>
<img data-src="assets/plots2/eigen_faces_data.png" alt="Training data is the Olivetti Faces corpus." /><figcaption aria-hidden="true">Training data is the Olivetti Faces corpus.</figcaption>
</figure>
<aside class="notes">
<p>The face data has been <em>crudely</em> aligned - meaning the major features, eyes nose, mouth etc are in corresponding locations in each image. The images are 64px square in this case.</p>
</aside>
</section>
<section id="eigenfaces-4" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<p>Recall, to reconstruct using PCA:</p>
<p><span class="math display">\[\mathbf{x} \approx \mathbf{\overline x} + \mathbf{P} \mathbf{b}\]</span></p>
</section>
<section id="eigenfaces-5" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<p>A human face can be approximated from the mean shape plus a linear combination of the eigenfaces.</p>
</section>
<section id="eigenfaces-6" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<figure>
<img data-src="assets/plots2/eigen_faces_mean.png" style="width:50.0%" alt="mean face" /><figcaption aria-hidden="true">mean face</figcaption>
</figure>
</section>
<section id="eigenfaces-7" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<figure>
<img data-src="assets/plots2/eigen_faces_components.png" alt="The Eigenfaces are the principal components of the data" /><figcaption aria-hidden="true">The <strong>Eigenfaces</strong> are the principal components of the data</figcaption>
</figure>
</section>
<section id="eigenfaces-8" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Eigenfaces</h2>
<figure>
<img data-src="assets/plots2/eigen_reconstruct.gif" style="width:90.0%" alt="Example reconstruction." /><figcaption aria-hidden="true">Example reconstruction.</figcaption>
</figure>
<aside class="notes">
<p>I feel this is quite impressive to be able to reconstruct a decent face from 150 or so parameters.</p>
</aside>
</section></section>
<section>
<section id="appearance-models" class="title-slide slide level1">
<h1>Appearance Models</h1>

</section>
<section id="appearance-models-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p>There is problem with the Eigenface approach:</p>
<div>
<ul>
<li class="fragment">There are <em>two</em> sources of variation - shape <strong>and</strong> appearance.</li>
<li class="fragment">One model is trying to capture both.</li>
<li class="fragment">We see <em>ghosting</em> in the images when reconstructed.</li>
</ul>
</div>
</section>
<section id="appearance-models-2" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p>We should model only the <em>appearance</em> variation.</p>
<p>A PDM is already able to model the shape.</p>
</section>
<section id="appearance-models-3" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p>Each <em>pixel</em> should represent the same <em>feature</em>.</p>
<p>We can’t achieve this goal by merely normalising the crop.</p>
<aside class="notes">
<p>I mean each pixel is a feature in our feature vector. I’m not referring to facial features.</p>
</aside>
</section>
<section id="appearance-models-4" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p>Given the hand-labelled images that were used to build the PDM, warp the images from the hand labels to the mean shape.</p>
<p>There are many ways to perform the warp, e.g.:</p>
<ul>
<li>Triangulate the landmarks, then map the pixels accordingly.</li>
</ul>
<aside class="notes">
<p>You can use Delauney triangulation to do this. Or, do it manually. It does not have to be the mean shape - we could say a canonical shape.</p>
</aside>
</section>
<section id="appearance-models-5" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p><img data-src="assets/plots2/triangulate_lmks.png" style="width:90.0%" /></p>
</section>
<section id="appearance-models-6" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p>For each image in the training data, and for each triangle:</p>
<div>
<ul>
<li class="fragment">Find <span class="math inline">\(M\)</span> for <span class="math inline">\(MA = B\)</span>, where <span class="math inline">\(A\)</span> is a triangle in an image and <span class="math inline">\(B\)</span> is the corresponding triangle in the mean shape.</li>
<li class="fragment">Use <span class="math inline">\(M\)</span> to warp the <em>pixels</em> in each triangle.</li>
<li class="fragment">Accumulate the patches into one shape normalised image.</li>
</ul>
</div>
<aside class="notes">
<p>by adding an homogeneous coordinate to the triangle, we can find the affine transform.</p>
</aside>
</section>
<section id="appearance-models-7" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p><img data-src="assets/plots2/mouth_triangulation.png" style="width:90.0%" /></p>
</section>
<section id="appearance-models-8" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p><img data-src="assets/plots2/shape_norm_1.png" style="width:90.0%" /></p>
</section>
<section id="appearance-models-9" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p><img data-src="assets/plots2/shape_norm_2.png" style="width:90.0%" /></p>
</section>
<section id="appearance-models-10" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p><img data-src="assets/plots2/shape_norm_3.png" style="width:90.0%" /></p>
</section>
<section id="appearance-models-11" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<p><img data-src="assets/plots2/shape_norm_4.png" style="width:90.0%" /></p>
</section>
<section id="appearance-models-12" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="assets/img4/8_mouths.png" /></p>
</div><div class="column">
<p>Images are shape-normalised.</p>
<ul>
<li>They all have the same number of pixels.</li>
<li>Each pixel represents the same feature.</li>
</ul>
</div>
</div>
</section>
<section id="appearance-models-13" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Appearance Models</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="assets/img4/8_mouths.png" /></p>
</div><div class="column">
<p>Applying PCA to the shape-normalised images gives a better model of appearance.</p>
<ul>
<li>The shape model and the appearance model can be combined or concatenated.</li>
<li>The appearance can provide a photo-metric loss for model fitting.</li>
</ul>
</div>
</div>
</section></section>
<section>
<section id="effectiveness-of-visual-features" class="title-slide slide level1">
<h1>Effectiveness of Visual Features</h1>

</section>
<section id="effectiveness" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Effectiveness</h2>
<p>Potamianos et al. (1998) compared visual features for automatic lip-reading.</p>
<ul>
<li>They trained models for a single talker reciting connected digits and measured accuracy as % correct.</li>
</ul>
</section>
<section id="effectiveness-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Effectiveness</h2>
<div style="display:block; font-size:0.75em">
<table>
<thead>
<tr class="header">
<th>Feature Class</th>
<th>Feature</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Articulatory</td>
<td>Height and Width</td>
<td>55.8%</td>
</tr>
<tr class="even">
<td></td>
<td>and Area</td>
<td>61.9%</td>
</tr>
<tr class="odd">
<td></td>
<td>and Perimeter</td>
<td>64.7%</td>
</tr>
<tr class="even">
<td>Fourier Descriptors</td>
<td>Outer Lip Contour</td>
<td>73.4%</td>
</tr>
<tr class="odd">
<td></td>
<td>Inner Lip Contour</td>
<td>64.0%</td>
</tr>
<tr class="even">
<td></td>
<td>Both Contours</td>
<td>83.9%</td>
</tr>
<tr class="odd">
<td>Appearance</td>
<td>LDA - based features</td>
<td>97.0%</td>
</tr>
</tbody>
</table>
</div>
<aside class="notes">
<p>LDA Linear Discriminant Analysis is a statistical method for dimensionality reduction.</p>
</aside>
</section>
<section id="effectiveness-2" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Effectiveness</h2>
<div class="columns">
<div class="column" style="width:65%;">
<p><img data-src="assets/img4/YLan2010.png" /></p>
</div><div class="column">
<p>Lan et al. (2010) compared visual features for automatic lip-reading.</p>
<div>
<ul>
<li class="fragment">Appearance (app)</li>
<li class="fragment">PDM (shape)</li>
<li class="fragment">DCT (2ddct)</li>
<li class="fragment">Eigenface (eigen-lip)</li>
<li class="fragment">Shape <strong>and</strong> Appearance (aam-pca)</li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="model-fusion" class="title-slide slide level1">
<h1>Model Fusion</h1>

</section>
<section id="audiovisual-fusion" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Audiovisual Fusion</h2>
<p>The acoustic and visual information needs to be combined - how and where this happens is important.</p>
<ul>
<li>We require that the performance after fusion is not worse than best performing individual modality.</li>
</ul>
</section>
<section id="audiovisual-fusion-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Audiovisual Fusion</h2>
<p>Two strategies:</p>
<div>
<ul>
<li class="fragment">Early integration: fusion is prior to recognition, e.g. at the feature level.</li>
<li class="fragment">Late integration: fusion is after recognition, e.g. sentence-level.</li>
</ul>
</div>
</section>
<section id="audiovisual-fusion-2" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Audiovisual Fusion</h2>
<p>Usually an estimation of the respective confidence is required.</p>
<div>
<ul>
<li class="fragment">Could be fixed, where it is learned during training based on accuracy.</li>
<li class="fragment">Could be adaptive, where it reflects the noise in the respective channels.</li>
</ul>
</div>
</section>
<section id="early-integration" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Early Integration</h2>
<p>Advantage of early integration:</p>
<ul>
<li>We can simply concatenate the features.</li>
<li>The structure of the recogniser does not need to change.</li>
</ul>
</section>
<section id="early-integration-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Early Integration</h2>
<p>Disadvantages of early integration:</p>
<ul>
<li>Size of the feature vector increases making training more difficult.</li>
<li>Need to normalise the features from different modalities, or weight them appropriately</li>
<li>Acoustic noise affects all of the input feature vectors.</li>
<li>Need to worry about the data rate, which might be different for both modalities.</li>
</ul>
</section>
<section id="late-integration" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Late Integration</h2>
<p>Advantages of late integration:</p>
<ul>
<li>Acoustic noise will not affect the visual recogniser.</li>
<li>Easier to adapt the recogniser to different conditions.</li>
<li>Notionally less training data are required to train the respective models.</li>
<li>Extends the structure of existing recognisers.</li>
<li>Either fuse the outputs or use a <em>multi-stream HMM</em>.</li>
</ul>
</section>
<section id="late-integration-1" class="slide level2" data-auto-animate="true">
<h2 data-auto-animate="true">Late Integration</h2>
<p>Disadvantages of late integration:</p>
<ul>
<li>May need to worry about the data rate, which is different for both modalities</li>
<li>Introduces extra parameters to optimise during training.</li>
</ul>
</section></section>
<section id="summary" class="title-slide slide level1">
<h1>Summary</h1>
<p>Integrating visual information can improve the robustness of speech recognisers to acoustic noise.</p>
<p>Face encoding using:</p>
<ul>
<li>DCT Features</li>
<li>Eigenfaces</li>
<li>Appearance Models</li>
</ul>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="https://unpkg.com/reveal.js@^4/plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4/plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4/plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4/plugin/math/math.js"></script>
  <script src="https://unpkg.com/reveal.js@^4/plugin/highlight/highlight.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,
        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',
        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: 'c/t',
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',
        // Turns fragments on and off globally
        fragments: true,
        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Global override for autoplaying embedded media (video/audio/iframe)
        // - null: Media will only autoplay if data-autoplay is present
        // - true: All media will autoplay, regardless of individual setting
        // - false: No media will autoplay, regardless of individual setting
        autoPlayMedia: null,
        // Global override for preloading lazy-loaded iframes
        // - null: Iframes with data-src AND data-preload will be loaded when within
        //   the viewDistance, iframes with only data-src will be loaded when visible
        // - true: All iframes with data-src will be loaded when within the viewDistance
        // - false: All iframes with data-src will be loaded only when visible
        preloadIframes: null,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,
        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,
        // Hide cursor if inactive
        hideInactiveCursor: true,
        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,
        // Factor of the display size that should remain empty around the content
        margin: 0.2,
        // The display mode that will be used to show slides
        display: 'block',
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealHighlight,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>